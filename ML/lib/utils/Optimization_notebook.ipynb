{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Optimizers for models that use gradient methods for finding the \n",
    "# weights that minimizes the loss.\n",
    "#resource: http://sebastianruder.com/optimizing-gradient-descent/index.html\n",
    "\n",
    "class GradientDescent():\n",
    "    def __init__(self,learningRate=0.01,momentum=0):\n",
    "        self.learningRate = learningRate\n",
    "        self.momentum     = momentum\n",
    "        self.w_update     = np.array([])\n",
    "        \n",
    "    def update(self, w, grad_w):\n",
    "        # For first update\n",
    "        if self.w_update.any():\n",
    "            self.w_update = np.zeros(w.shape)\n",
    "        # include momentum to speed up learning\n",
    "        self.w_update = self.momentum * self.w_update + grad_w\n",
    "        # Move along gradient\n",
    "        return w - self.learningRate*self.w_update\n",
    "    \n",
    "class Adam():\n",
    "    # Favorable to other techniques\n",
    "    def __init__(self,learningRate=0.001,b1=0.9,b2=0.999):\n",
    "        self.learningRate = learningRate\n",
    "        self.eps = 1e-8\n",
    "        # mean/uncentered variance of gradients\n",
    "        self.m = np.array([])\n",
    "        self.v = np.array([]) \n",
    "        # Decay rates (suggested by authors of ADAM optimizer)\n",
    "        self.b1 = b1 \n",
    "        self.b2 = b2\n",
    "\n",
    "    def update(self, w, grad_w):\n",
    "\n",
    "        # Initialize \n",
    "        if not self.m.any():\n",
    "            self.m = np.zeros(np.shape(grad_w))\n",
    "            self.v = np.zeros(np.shape(grad_w))\n",
    "        # Update mean/variance of gradients w/ exponential decaying average\n",
    "        self.m = self.b1 * self.m + (1 - self.b1) * grad_w\n",
    "        self.v = self.b2 * self.v + (1 - self.b2) * np.power(grad_w, 2)\n",
    "        # Counteract bias toward 0 (when decay rates are small)\n",
    "        m_hat = self.m / (1 - self.b1)\n",
    "        v_hat = self.v / (1 - self.b2)\n",
    "        # Update rule\n",
    "        self.w_update = self.learningRate / (np.sqrt(v_hat) + self.eps) * m_hat\n",
    "\n",
    "        return w - self.w_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
